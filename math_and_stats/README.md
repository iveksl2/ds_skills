ML - Best Practices:
  * [The First Rule of Machine Learning: Start without Machine Learning - Eugene Yan](https://eugeneyan.com/writing/first-rule-of-ml/)
  
ML-Theory:
  * [How Backpropogation Works - Brandon Rohrer](https://www.youtube.com/watch?v=6BMwisTZFr4)
  
Stats: 
  * [CLT - Nassim Taleb](https://youtu.be/bfM9efdStN8?t=179) 
    * If you sample from any distribution with finite variance, the sum of samples will approach a normal distribution. If the variance isn't contant it will approach a taylor distribution. 
  * [Dance of the P-values](https://www.youtube.com/watch?v=ez4DgdurRPg) 
  * [Bayesians have 'won' datascience - Brian Godsey](https://towardsdatascience.com/the-bayesians-have-won-data-science-caa3ef4672b4) (I don't necessarily agree but some interesting points on properties)
    * Frequentist express probability as the 'expectedfrequency of an event', bayesians 'degree of belief'
    * Bayesian don't need to retrain on the full dataset with new data given the prior distribution
    * Bayesian models propagate uncertainty 

Sick Reference:
  * [Papers With Code](https://portal.paperswithcode.com/)
